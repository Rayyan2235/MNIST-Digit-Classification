{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "akaPmL1Ya_LL"
      },
      "source": [
        "# MNIST Handwritten Digit Classification with Neural Networks\n",
        "\n",
        "## Introduction\n",
        "\n",
        "Welcome to the second MLB project! In this notebook, we'll build a **Neural Network** to recognize handwritten digits from the famous MNIST dataset.\n",
        "\n",
        "### What is MNIST?\n",
        "MNIST (Modified National Institute of Standards and Technology) is a dataset containing 70,000 images of handwritten digits (0-9). Each image is 28x28 pixels in grayscale.\n",
        "\n",
        "### What Will We Learn?\n",
        "- How to load and explore image data\n",
        "- How to preprocess data for neural networks\n",
        "- How to build a neural network from scratch\n",
        "- How to train and evaluate our model\n",
        "- How to make predictions on new data\n",
        "\n",
        "Let's get started! üöÄ"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "k-fRXfuwa_LM"
      },
      "source": [
        "## Step 1: Import Required Libraries\n",
        "\n",
        "We'll need several libraries for this project:\n",
        "- **numpy**: For numerical operations and array manipulation\n",
        "- **pandas**: For data organization and analysis\n",
        "- **matplotlib & seaborn**: For creating visualizations\n",
        "- **tensorflow/keras**: For building our neural network\n",
        "- **sklearn**: For splitting data and evaluation metrics"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ABUa2vtUa_LM"
      },
      "outputs": [],
      "source": [
        "# Import essential libraries\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "import matplotlib.pyplot as plt\n",
        "import seaborn as sns\n",
        "\n",
        "# For neural network building\n",
        "import tensorflow as tf\n",
        "from tensorflow import keras\n",
        "from tensorflow.keras import layers\n",
        "from tensorflow.keras.models import Sequential\n",
        "from tensorflow.keras.layers import Dense, Flatten, Dropout\n",
        "from tensorflow.keras.utils import to_categorical\n",
        "\n",
        "# For data preprocessing and evaluation\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.metrics import confusion_matrix, classification_report, accuracy_score\n",
        "\n",
        "# Set visualization style. We use these libraries to build graphs, as visualizing the data is often\n",
        "# an important step in ML!\n",
        "sns.set_style(\"whitegrid\")\n",
        "plt.rcParams['figure.figsize'] = (12, 8)\n",
        "\n",
        "# Set random seed for reproducibility. All this is doing is making sure we select the random sequences in a\n",
        "# way that can be reproduced again so we can compare results.\n",
        "np.random.seed(42)\n",
        "tf.random.set_seed(42)\n",
        "\n",
        "print(\"‚úÖ All libraries imported successfully!\")\n",
        "print(f\"TensorFlow version: {tf.__version__}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "BNlaFNh3a_LM"
      },
      "source": [
        "## Step 2: Load the MNIST Dataset\n",
        "\n",
        "The MNIST dataset comes pre-loaded with Keras, making it easy to access. We'll load both training and test data!\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "h0zACVWOa_LM"
      },
      "outputs": [],
      "source": [
        "# Load MNIST dataset from Keras\n",
        "# This automatically splits the data into training and testing sets, so you don't have to do that manually yourself!\n",
        "(X_train, y_train), (X_test, y_test) = keras.datasets.mnist.load_data()\n",
        "\n",
        "\n",
        "# -- TODO --\n",
        "\n",
        "# Print out the datasets shape here, including the training and testing sets. This is an important step because the dataset's\n",
        "# shape, or the number of samples and the dimensions it spans is very important to know. For example, remember that the input\n",
        "# layer of our NN has to match the number of pixels in our image! You would not be able to tell this if you didn't know\n",
        "# the shape of the data you are working with.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ZJdSTTL6a_LN"
      },
      "source": [
        "## Step 3: Explore the Data\n",
        "\n",
        "Before building our model, let's understand what we're working with. We'll:\n",
        "1. Check the distribution of digits in our dataset\n",
        "2. Visualize some example images\n",
        "3. Examine pixel value ranges"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "FK5uf-Jya_LN"
      },
      "outputs": [],
      "source": [
        "# Here are some cool visualizations we can do with the data to see the distribution of digits in our dataset.\n",
        "# Data visualization is another cool part of ML and gives us a powerful visual of the data we are working with.\n",
        "# We gave you an example of this, but feel free to add a cell below here to explore things such as the distrubution\n",
        "# of numbers, etc! Let your curiosity take over.\n",
        "\n",
        "\n",
        "plt.figure(figsize=(15, 6))\n",
        "for i in range(20):\n",
        "    plt.subplot(4, 5, i + 1)\n",
        "    plt.imshow(X_train[i], cmap='gray')\n",
        "    plt.title(f'Label: {y_train[i]}', fontsize=10)\n",
        "    plt.axis('off')\n",
        "\n",
        "plt.suptitle('Sample Images from MNIST Dataset', fontsize=16, fontweight='bold', y=1.02)\n",
        "plt.tight_layout()\n",
        "plt.show()\n",
        "\n",
        "print(\"\\nüí° These are the actual handwritten digits our model will learn to recognize!\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Q6-JnewYa_LN"
      },
      "source": [
        "## Step 4: Data Preprocessing\n",
        "\n",
        "Neural networks work best when data is properly prepared. We'll perform three key preprocessing steps:\n",
        "\n",
        "### 4.1 Normalization\n",
        "We'll scale pixel values from [0, 255] to [0, 1]. This helps the neural network learn faster and more effectively.\n",
        "\n",
        "**Why normalize?**\n",
        "- Neural networks work better with smaller values\n",
        "- It prevents certain features from dominating others\n",
        "- It helps the model converge faster during training. This is because it won't have to deal with the more complicated calculations! Binary ones allow us to speed up the training and makes things a little simpler."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "w68ChHyva_LN"
      },
      "outputs": [],
      "source": [
        "# Right now, all of the images have pixel values between 0 and 255. We want to change this!\n",
        "\n",
        "# -- TODO --\n",
        "# Normalize all the pixel values, so that they are between 0 and 1. (HINT: Take the X_train and X_test values and divide them by 255.0! Make sure\n",
        "# to cast these as different variables, so you don't them mixed up!)\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "w_I2MyExa_LN"
      },
      "source": [
        "### 4.2 One-Hot Encoding\n",
        "\n",
        "Our labels, or what we are predicing, are currently numbers (0-9). This makes sense right, after all, the point of the NN is to convert an image ot a number between 0-9. However, we need to convert them to what are called **one-hot encoded** vectors.\n",
        "\n",
        "This is because we need the labels, or what is predicted, to match the output format of the NN. If you remember in the NN MLB, we told you that if we are predicted 10 digits, we needed the final layer of the NN to have 10 nodes! By making sure the labels are encoded, we make sure to give the NN a clear format to predict.\n",
        "\n",
        "Here is a great short video to learn a little more about this: https://www.youtube.com/watch?v=G2iVj7WKDFk\n",
        "\n",
        "**Example:**\n",
        "- Label 3 becomes: [0, 0, 0, 1, 0, 0, 0, 0, 0, 0]\n",
        "- Label 7 becomes: [0, 0, 0, 0, 0, 0, 0, 1, 0, 0]\n",
        "\n",
        "**Why one-hot encode?**\n",
        "- Neural networks output probabilities for each class\n",
        "- It prevents the model from thinking there's an ordinal relationship between digits\n",
        "- It matches the output format of our neural network"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "uqRpibSRa_LN"
      },
      "outputs": [],
      "source": [
        "# -- TODO --\n",
        "\n",
        "# Convert the labels, or the y_train and y_test values using one-hot encoding! (HINT: use the to_categorical() method)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "0Ej4RDkwa_LN"
      },
      "source": [
        "### 4.3 Data Reshaping\n",
        "\n",
        "Our images are currently 28x28, or a 2D, matrices. For a basic neural network, we need to flatten them into a huge 1D vector of 784 pixels (28 √ó 28 = 784). Remember, this is what is going into the input layer of the NN!"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "LeasUxFCa_LO"
      },
      "outputs": [],
      "source": [
        "# -- TODO --\n",
        "\n",
        "# Use your normalized X_train and X_test arrays and use the \".reshape()\" method to change these arrays into a 1D vector\n",
        "# with a size of 28 * 28. Make sure to cast these as new variables!\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "pCgsNGS2a_LO"
      },
      "source": [
        "## Step 5: Build the Neural Network\n",
        "\n",
        "Now comes the exciting part - building our neural network!\n",
        "\n",
        "### Architecture Overview:\n",
        "Our neural network will have:\n",
        "1. **Input Layer**: 784 neurons (one for each pixel)\n",
        "2. **Hidden Layer 1**: 128 neurons with ReLU activation\n",
        "3. **Dropout Layer**: Prevents overfitting (20% dropout). Don't worry too much about this, it's just good practice to use dropout to prevent our NN from memorizing the data instead of patterns in the data.\n",
        "4. **Hidden Layer 2**: 64 neurons with ReLU activation\n",
        "5. **Output Layer**: 10 neurons with Softmax activation (one for each digit). Softmax is an activation function, much like ReLU that gives us a probability distribution of which of the 10 digits our image could be!\n",
        "\n",
        "### Key Concepts:\n",
        "- **ReLU (Rectified Linear Unit)**: Activation function that introduces non-linearity\n",
        "- **Dropout**: Randomly turns off neurons during training to prevent overfitting\n",
        "- **Softmax**: Converts output to probabilities that sum to 1"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "d-vN55A6a_LO"
      },
      "outputs": [],
      "source": [
        "# -- TODO --\n",
        "\n",
        "# Build the neural network architecture! Define a variable called \"model\" (for convention, but you can call it whatever of course!)\n",
        "# Use a \"Sequential\" model class to build out the NN. Make sure to include 3 layers in the following format:\n",
        "# Dense -> Dropout -> Dense -> Dense.\n",
        "\n",
        "# We'll leave it up to you to figure out the input shape and sizes! Definetely feel free to seek out some help building the model\n",
        "# out, it can be a bit tricky at first!\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "8C4J7PWia_LO"
      },
      "source": [
        "### Understanding the Model Summary:\n",
        "\n",
        "- **Param #**: Total number of learnable parameters (weights and biases)\n",
        "- **Output Shape**: Dimensions of data after each layer\n",
        "- Layer 1: 784 inputs √ó 128 outputs + 128 biases = 100,480 parameters\n",
        "- Layer 2: 128 inputs √ó 64 outputs + 64 biases = 8,256 parameters\n",
        "- Output: 64 inputs √ó 10 outputs + 10 biases = 650 parameters\n",
        "\n",
        "**Total**: ~109K parameters that the model will learn!"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "YtpKwo0Ca_LO"
      },
      "source": [
        "## Step 6: Compile the Model\n",
        "\n",
        "Before training, we need to configure the learning process by specifying:\n",
        "\n",
        "1. **Optimizer**: Algorithm that updates weights (Adam - adaptive learning rate)\n",
        "2. **Loss Function**: How we measure model performance (categorical crossentropy)\n",
        "3. **Metrics**: What we track during training (accuracy)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "M2yHEvFfa_LO"
      },
      "outputs": [],
      "source": [
        "# Now that we have build out our model, we can compile it with optimizer, loss function, and metrics.\n",
        "# These are just aspects of our model that will help us train the model more accurately and are\n",
        "# needed!\n",
        "model.compile(\n",
        "    optimizer='adam',                      # Adaptive learning rate optimizer\n",
        "    loss='categorical_crossentropy',       # Loss function for multi-class classification\n",
        "    metrics=['accuracy']                   # Track accuracy during training\n",
        ")\n",
        "\n",
        "print(\"Model compiled successfully!\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "nwek5lbsa_LO"
      },
      "source": [
        "## Step 7: Train the Model\n",
        "\n",
        "Now we'll train our neural network! During training:\n",
        "- The model learns patterns from the training data\n",
        "- We validate on a portion of data to prevent overfitting\n",
        "- We track loss and accuracy over multiple epochs\n",
        "\n",
        "**Hyperparameters:**\n",
        "- **Epochs**: Number of complete passes through the training data (15)\n",
        "- **Batch Size**: Number of samples processed before updating weights (128)\n",
        "- **Validation Split**: Percentage of training data used for validation (20%)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "2XnB_oN7a_LO"
      },
      "outputs": [],
      "source": [
        "# -- TODO --\n",
        "\n",
        "# Now comes the final part! We're finally ready to train our model. For our purposes, we can use ~15 epochs, or the number of\n",
        "# training iterations, a batch size of ~128 and a validation split of 0.2, which means that we can use 20% for validating, which\n",
        "# is basically checking our model as it trains to check for overfitting. We don't usually learn from this, it's just something for us\n",
        "# to keep track."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "hopdHS-Ha_LO"
      },
      "source": [
        "## Step 8: Visualize Training History\n",
        "\n",
        "Let's see how our model performed during training by plotting:\n",
        "1. **Accuracy**: How often the model predicts correctly\n",
        "2. **Loss**: How far predictions are from actual values\n",
        "\n",
        "**What to look for:**\n",
        "- Training and validation curves should be close together\n",
        "- Both should improve over time\n",
        "- Large gaps indicate overfitting"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "OYkrSWQta_LO"
      },
      "outputs": [],
      "source": [
        "# Here, we've given you a couple of graphs to help you see the model as it trained over time! (NOTE: the variable \"history\" just refers\n",
        "# to the model itself, you might have called it something different).\n",
        "\n",
        "# Plot training history\n",
        "fig, axes = plt.subplots(1, 2, figsize=(15, 5))\n",
        "\n",
        "# Plot accuracy\n",
        "axes[0].plot(history.history['accuracy'], label='Training Accuracy', linewidth=2)\n",
        "axes[0].plot(history.history['val_accuracy'], label='Validation Accuracy', linewidth=2)\n",
        "axes[0].set_title('Model Accuracy Over Time', fontsize=14, fontweight='bold')\n",
        "axes[0].set_xlabel('Epoch', fontsize=12)\n",
        "axes[0].set_ylabel('Accuracy', fontsize=12)\n",
        "axes[0].legend(fontsize=10)\n",
        "axes[0].grid(alpha=0.3)\n",
        "\n",
        "# Plot loss\n",
        "axes[1].plot(history.history['loss'], label='Training Loss', linewidth=2)\n",
        "axes[1].plot(history.history['val_loss'], label='Validation Loss', linewidth=2)\n",
        "axes[1].set_title('Model Loss Over Time', fontsize=14, fontweight='bold')\n",
        "axes[1].set_xlabel('Epoch', fontsize=12)\n",
        "axes[1].set_ylabel('Loss', fontsize=12)\n",
        "axes[1].legend(fontsize=10)\n",
        "axes[1].grid(alpha=0.3)\n",
        "\n",
        "plt.tight_layout()\n",
        "plt.show()\n",
        "\n",
        "# Print final training metrics\n",
        "final_train_acc = history.history['accuracy'][-1]\n",
        "final_val_acc = history.history['val_accuracy'][-1]\n",
        "print(f\"\\nüìä Final Training Accuracy: {final_train_acc:.4f} ({final_train_acc*100:.2f}%)\")\n",
        "print(f\"üìä Final Validation Accuracy: {final_val_acc:.4f} ({final_val_acc*100:.2f}%)\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "xGh-3DQha_LO"
      },
      "source": [
        "## Step 9: Evaluate Model Performance\n",
        "\n",
        "Let's test our model on completely unseen data (test set) to see how well it generalizes.\n",
        "\n",
        "We'll calculate:\n",
        "- **Overall accuracy**\n",
        "- **Loss value**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "tGGnjjToa_LO"
      },
      "outputs": [],
      "source": [
        "# -- TODO --\n",
        "# Now that we've trained our model and the accuracy (hopefully) looks promising, let's test our model on our test set of data now.\n",
        "# HINT: Use the .evalutate() method to evaluate the model on the testing dataset.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "HRC6jxZ4t0gt"
      },
      "outputs": [],
      "source": [
        "# -- TODO --\n",
        "\n",
        "# Now, let's make predictions using our model on the 1D flattened testing values. All you are doing here is using your trained\n",
        "# model to make predictions on the X_test flattened array.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "4g0x4HgAu5xB"
      },
      "source": [
        "## Step 10: Visualize Predictions\n",
        "\n",
        "Let's see our model in action! We'll display:\n",
        "1. Sample images from the test set\n",
        "2. Model's predictions\n",
        "3. Actual labels\n",
        "4. Prediction confidence\n",
        "\n",
        "We've provided these for you, but note that the variable names might be different here!\n",
        "\n",
        "This part has been completed for you and is not graded!\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Dxbq4znLu63W"
      },
      "outputs": [],
      "source": [
        "# Visualize correct predictions. Again, our variables for y_pred and y_test might be different from yours, just be aware!\n",
        "correct_indices = np.where(y_pred == y_test)[0]\n",
        "sample_correct = np.random.choice(correct_indices, 12, replace=False)\n",
        "\n",
        "plt.figure(figsize=(15, 8))\n",
        "for i, idx in enumerate(sample_correct):\n",
        "    plt.subplot(3, 4, i + 1)\n",
        "    plt.imshow(X_test[idx], cmap='gray')\n",
        "    confidence = y_pred_probs[idx][y_pred[idx]] * 100\n",
        "    plt.title(f'Pred: {y_pred[idx]} (True: {y_test[idx]})\\nConfidence: {confidence:.1f}%',\n",
        "              fontsize=10, color='green')\n",
        "    plt.axis('off')\n",
        "\n",
        "plt.suptitle('Correct Predictions with Confidence Scores',\n",
        "             fontsize=16, fontweight='bold', color='green', y=1.02)\n",
        "plt.tight_layout()\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "h0C9nbIyu9BX"
      },
      "outputs": [],
      "source": [
        "# Visualize incorrect predictions\n",
        "incorrect_indices = np.where(y_pred != y_test)[0]\n",
        "\n",
        "if len(incorrect_indices) > 0:\n",
        "    sample_incorrect = np.random.choice(incorrect_indices,\n",
        "                                       min(12, len(incorrect_indices)),\n",
        "                                       replace=False)\n",
        "\n",
        "    plt.figure(figsize=(15, 8))\n",
        "    for i, idx in enumerate(sample_incorrect):\n",
        "        plt.subplot(3, 4, i + 1)\n",
        "        plt.imshow(X_test[idx], cmap='gray')\n",
        "        confidence = y_pred_probs[idx][y_pred[idx]] * 100\n",
        "        plt.title(f'Pred: {y_pred[idx]} (True: {y_test[idx]})\\nConfidence: {confidence:.1f}%',\n",
        "                  fontsize=10, color='red')\n",
        "        plt.axis('off')\n",
        "\n",
        "    plt.suptitle('Incorrect Predictions - Learning Opportunities!',\n",
        "                 fontsize=16, fontweight='bold', color='red', y=1.02)\n",
        "    plt.tight_layout()\n",
        "    plt.show()\n",
        "\n",
        "    print(f\"\\nüí° Total misclassifications: {len(incorrect_indices)} out of {len(y_test)}\")\n",
        "    print(f\"Error rate: {len(incorrect_indices)/len(y_test)*100:.2f}%\")\n",
        "else:\n",
        "    print(\"\\nüéâ Wow! Perfect predictions on all test samples!\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "E0qqCyaKa_LP"
      },
      "source": [
        "## Step 11: Interactive Prediction\n",
        "\n",
        "Try predicting different images by changing the index below. Experiment with different test samples to see how your model performs! Again, this is just for your visualization, this won't be graded and there isn't anything you have to do here!"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Uud9FPkya_LP"
      },
      "outputs": [],
      "source": [
        "# Function to predict a single image\n",
        "def predict_digit(image_index):\n",
        "    \"\"\"\n",
        "    Predict the digit for a given test image and display the result.\n",
        "\n",
        "    Parameters:\n",
        "    -----------\n",
        "    image_index : int\n",
        "        Index of the test image to predict\n",
        "    \"\"\"\n",
        "    # Get the image and reshape for model input\n",
        "    image = X_test_normalized[image_index].reshape(1, 28 * 28)\n",
        "\n",
        "    # Make prediction\n",
        "    prediction_probs = model.predict(image, verbose=0)\n",
        "    predicted_digit = np.argmax(prediction_probs)\n",
        "    confidence = prediction_probs[0][predicted_digit] * 100\n",
        "\n",
        "    # Display the image and prediction\n",
        "    plt.figure(figsize=(6, 6))\n",
        "    plt.imshow(X_test[image_index], cmap='gray')\n",
        "    plt.title(f'Predicted: {predicted_digit}\\nActual: {y_test[image_index]}\\nConfidence: {confidence:.2f}%',\n",
        "              fontsize=14, fontweight='bold')\n",
        "    plt.axis('off')\n",
        "    plt.show()\n",
        "\n",
        "    # Print probability distribution\n",
        "    print(\"\\nProbability Distribution:\")\n",
        "    print(\"-\" * 30)\n",
        "    for digit in range(10):\n",
        "        prob = prediction_probs[0][digit] * 100\n",
        "        bar = \"‚ñà\" * int(prob / 2)\n",
        "        print(f\"Digit {digit}: {prob:5.2f}% {bar}\")\n",
        "\n",
        "    return predicted_digit\n",
        "\n",
        "# Test the function with a random image\n",
        "random_index = np.random.randint(0, len(X_test))\n",
        "print(f\"Testing with image at index {random_index}:\\n\")\n",
        "predicted = predict_digit(random_index)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "KsDJEhSAa_LP"
      },
      "outputs": [],
      "source": [
        "test_index = 42\n",
        "\n",
        "print(f\"\\nPredicting image at index {test_index}:\\n\")\n",
        "predict_digit(test_index)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "7VNEV2NTa_LP"
      },
      "source": [
        "## Step 13: Model Summary and Key Insights\n",
        "\n",
        "Let's review what we've accomplished and discuss potential improvements. Again, the variable names might be different here, so \n",
        "just keep that in mind."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "DGhXQoGja_LP"
      },
      "outputs": [],
      "source": [
        "# Summary of model performance\n",
        "print(\"\\n\" + \"=\"*70)\n",
        "print(\"üéØ MODEL PERFORMANCE SUMMARY\")\n",
        "print(\"=\"*70)\n",
        "\n",
        "print(f\"\\nüìä Training Metrics:\")\n",
        "print(f\"   - Final Training Accuracy: {final_train_acc*100:.2f}%\")\n",
        "print(f\"   - Final Validation Accuracy: {final_val_acc*100:.2f}%\")\n",
        "\n",
        "print(f\"\\nüìä Test Set Performance:\")\n",
        "print(f\"   - Test Accuracy: {test_accuracy*100:.2f}%\")\n",
        "print(f\"   - Test Loss: {test_loss:.4f}\")\n",
        "\n",
        "print(f\"\\nüî¢ Model Architecture:\")\n",
        "print(f\"   - Total Parameters: {model.count_params():,}\")\n",
        "print(f\"   - Hidden Layers: 2\")\n",
        "print(f\"   - Neurons: [128, 64]\")\n",
        "print(f\"   - Dropout Rate: 20%\")\n",
        "\n",
        "print(f\"\\n‚öôÔ∏è Training Configuration:\")\n",
        "print(f\"   - Epochs: 15\")\n",
        "print(f\"   - Batch Size: 128\")\n",
        "print(f\"   - Optimizer: Adam\")\n",
        "print(f\"   - Loss Function: Categorical Crossentropy\")\n",
        "\n",
        "print(\"\\n\" + \"=\"*70)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "mhCRWVjva_LP"
      },
      "source": [
        "## Conclusion\n",
        "\n",
        "### Congratulations!\n",
        "\n",
        "By the end of this, you've successfully:\n",
        "- ‚úÖ Loaded and explored the MNIST dataset\n",
        "- ‚úÖ Preprocessed image data for neural networks\n",
        "- ‚úÖ Built a multi-layer neural network from scratch\n",
        "- ‚úÖ Trained the model and visualized its learning process\n",
        "- ‚úÖ Evaluated performance using multiple metrics\n",
        "- ‚úÖ Made predictions on new data\n",
        "\n",
        "### Some key takeaways:\n",
        "\n",
        "1. **Data Preprocessing is Critical:**\n",
        "   - Normalization helps neural networks learn faster\n",
        "   - One-hot encoding is essential for classification tasks\n",
        "   - Proper data formatting prevents training errors\n",
        "\n",
        "2. **Model Architecture Matters:**\n",
        "   - More layers/neurons ‚â† better performance always\n",
        "   - Dropout helps prevent overfitting\n",
        "   - The right architecture depends on your problem\n",
        "\n",
        "3. **Evaluation Goes Beyond Accuracy:**\n",
        "   - Confusion matrices reveal patterns in errors\n",
        "   - Precision and recall provide class-specific insights\n",
        "   - Visual inspection helps understand model behavior\n",
        "\n",
        "### Thank you for making it to the end of the second project!\n",
        "\n",
        "Keep practicing, stay curious, and happy learning! üöÄ\n",
        "\n",
        "---\n",
        "\n",
        "*Remember: The best way to learn machine learning is by doing. Don't be afraid to experiment, make mistakes, and iterate!*"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.8.0"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
